{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data set\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "print(\"Features: %s\" % iris.feature_names)\n",
    "print(\"Labels  : %s\" % iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.data.shape)\n",
    "print(iris.target.shape)\n",
    "\n",
    "n_samples, n_features = iris.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the data\n",
    "plt.figure(2, figsize=(8, 6))\n",
    "plt.clf()\n",
    "\n",
    "# iris.target[iris.target > 0] = 1  # distinguish only two kind of iris flowers\n",
    "\n",
    "plt.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target, cmap=plt.cm.Set1)\n",
    "\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data, again\n",
    "plt.figure(2, figsize=(8, 6))\n",
    "plt.clf()\n",
    "\n",
    "plt.scatter(iris.data[:, 2], iris.data[:, 3], c=iris.target, cmap=plt.cm.Set1)\n",
    "\n",
    "plt.xlabel('Petal length')\n",
    "plt.ylabel('Petal width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would include things like feature extraction, dealing with missing numbers, outlier detection, data normalization, etc. pp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# train a logistic regressor to classify the data\n",
    "clf = linear_model.LogisticRegression()\n",
    "\n",
    "# train a classifier\n",
    "clf.fit(iris.data[:-10], iris.target[:-10])\n",
    "\n",
    "# make predictions using the trained classifier\n",
    "predictions = clf.predict(iris.data[-10:])\n",
    "\n",
    "print(\"Predicted label(s): %s\" % predictions)\n",
    "print(\"True label(s): %s\" % iris.target[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ˜® ... Oops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling, again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data\n",
    "shuffle_index = np.random.permutation(n_samples)\n",
    "\n",
    "# use 10-20 percent of the data for testing, the rest for training\n",
    "split = int(n_samples * 0.1)\n",
    "\n",
    "# split the data into training and test sets\n",
    "test_idx = shuffle_index[:split]\n",
    "train_idx = shuffle_index[split:]\n",
    "\n",
    "X_train = iris.data[train_idx]\n",
    "X_test = iris.data[test_idx]\n",
    "y_train = iris.target[train_idx]\n",
    "y_test = iris.target[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new classifier\n",
    "clf = linear_model.LogisticRegression()  # C=1e5\n",
    "# clf = sklearn.svm.SVC(gamma=0.001, C=100.)\n",
    "\n",
    "# train a classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions using the trained classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Predicted labels: %s\" % y_pred)\n",
    "print(\"True labels     : %s\" % y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make it easier we will distinguish only two classes here\n",
    "# i.e. turn the problem into a so-called binary classification problem\n",
    "\n",
    "# make a new classifier\n",
    "clf = linear_model.LogisticRegression()\n",
    "\n",
    "y_train[y_train == 2] = 1\n",
    "y_test[y_test == 2] = 1\n",
    "\n",
    "# train a classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions using the trained classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Predicted labels: %s\" % y_pred)\n",
    "print(\"True labels     : %s\" % y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy\n",
    "\n",
    "The number of true positive plus the number of true negatives divided by the number of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of true positives and true negatives in one go\n",
    "true_predictions = len(y_pred[y_pred == y_test])\n",
    "print(true_predictions / len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1-Score\n",
    "\n",
    "Task: Calculate the number of false positives and the number of false negatives in the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"400px\" align=\"left\" src=\"files/fscore.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new classifier\n",
    "clf = linear_model.LogisticRegression()\n",
    "\n",
    "# get the original three classes again\n",
    "y_train = iris.target[train_idx]\n",
    "y_test = iris.target[test_idx]\n",
    "\n",
    "# train a classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions using the trained classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Predicted labels: %s\" % y_pred)\n",
    "print(\"True labels     : %s\" % y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = metrics.f1_score(y_test, y_pred, average=None)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task for who finished the rest ðŸ˜‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced snippet from http://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html\n",
    "\n",
    "# Task: understand the following code which plots the decision boundaries of the trained classifier for two features\n",
    "# and adapt it such that it shows the decision boundaries aka rules for the other two features.\n",
    "\n",
    "# load the data set\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only use the first two features because it's easier to visualize 2D than 4D\n",
    "Y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# instantiate a classifer and train it\n",
    "logreg = sklearn.linear_model.LogisticRegression(C=1e5)\n",
    "logreg.fit(X, Y)\n",
    "\n",
    "# plot the decision boundary\n",
    "# therefore assign a color to each point in the mesh [x_min, x_max]x[y_min, y_max]\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(2, figsize=(8, 6))\n",
    "plt.clf()\n",
    "\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# plot the training samples\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
